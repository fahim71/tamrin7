{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBqN02gPADTk",
        "outputId": "12f66286-d87a-4bd8-c82f-eba569ca3c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyspark==3.2.0 kafka-python==2.0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSgNKq9p7Fg_",
        "outputId": "0019e5e9-e1b9-4601-e24c-d803e6446829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark==3.2.0\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kafka-python==2.0.2\n",
            "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805917 sha256=6dfe8aca0c0c9ac796df7cab6e30817ecfd8e44f24962d82b7ba7397df3b8af6\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/97/d3/8b6d964c8700e4fbb561c71638a92ec55dac9be51eb5fea86d\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, kafka-python, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.4.0\n",
            "    Uninstalling pyspark-3.4.0:\n",
            "      Successfully uninstalled pyspark-3.4.0\n",
            "Successfully installed kafka-python-2.0.2 py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTMe3vTrAnZT"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col,count,when,isnan\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.clustering import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBHigtFiCYO7"
      },
      "outputs": [],
      "source": [
        "spark=SparkSession.builder.appName('my_app').getOrCreate()\n",
        "df=spark.read.csv('/content/uber-raw-data-aug14.csv',header=True,inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install kafka-python"
      ],
      "metadata": {
        "id": "tNoY-hmkA0r-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5805ccca-b5ab-49cf-c9de-821c765976a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IBeUpjtFXEm",
        "outputId": "ab926409-79f1-45bd-e10f-0d4f11c4d9cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------+--------+------+\n",
            "|       Date/Time|    Lat|     Lon|  Base|\n",
            "+----------------+-------+--------+------+\n",
            "|8/1/2014 0:03:00|40.7366|-73.9906|B02512|\n",
            "|8/1/2014 0:09:00| 40.726|-73.9918|B02512|\n",
            "|8/1/2014 0:12:00|40.7209|-74.0507|B02512|\n",
            "|8/1/2014 0:12:00|40.7387|-73.9856|B02512|\n",
            "|8/1/2014 0:12:00|40.7323|-74.0077|B02512|\n",
            "|8/1/2014 0:13:00|40.7349|-74.0033|B02512|\n",
            "|8/1/2014 0:15:00|40.7279|-73.9542|B02512|\n",
            "|8/1/2014 0:17:00| 40.721|-73.9937|B02512|\n",
            "|8/1/2014 0:19:00|40.7195| -74.006|B02512|\n",
            "|8/1/2014 0:20:00|40.7448|-73.9799|B02512|\n",
            "|8/1/2014 0:21:00|40.7399|-74.0057|B02512|\n",
            "|8/1/2014 0:25:00|40.7651|-73.9683|B02512|\n",
            "|8/1/2014 0:27:00|40.7354|-74.0081|B02512|\n",
            "|8/1/2014 0:29:00|40.7339|-74.0028|B02512|\n",
            "|8/1/2014 0:29:00|40.7364|-74.0301|B02512|\n",
            "|8/1/2014 0:29:00|40.7364|-74.0301|B02512|\n",
            "|8/1/2014 0:30:00|40.7252|-73.9516|B02512|\n",
            "|8/1/2014 0:30:00|40.7433| -73.986|B02512|\n",
            "|8/1/2014 0:34:00|40.7437|-73.9884|B02512|\n",
            "|8/1/2014 0:36:00|40.7406|-74.0077|B02512|\n",
            "+----------------+-------+--------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqfln26c2eXu",
        "outputId": "baf97cab-d51c-4a68-9b42-63f4b16daab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows:  829275\n",
            "Number of columns:  4\n"
          ]
        }
      ],
      "source": [
        "nr= df.count()\n",
        "nc = len(df.columns)\n",
        "print(\"Number of rows: \", nr)\n",
        "print(\"Number of columns: \", nc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbS-mE6AFaxE",
        "outputId": "ae534aae-bdec-49a6-d13c-282f55e4f621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+--------------------+-------------------+------+\n",
            "|summary|       Date/Time|                 Lat|                Lon|  Base|\n",
            "+-------+----------------+--------------------+-------------------+------+\n",
            "|  count|          829275|              829275|             829275|829275|\n",
            "|   mean|            null|   40.73778073582462| -73.97016031317625|  null|\n",
            "| stddev|            null|0.043628060846868946|0.06148272834518351|  null|\n",
            "|    min|8/1/2014 0:00:00|             39.6569|           -74.7737|B02512|\n",
            "|    max|8/9/2014 9:59:00|             41.3182|           -72.3359|B02764|\n",
            "+-------+----------------+--------------------+-------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgc3WRnUGZ_u",
        "outputId": "42bc0142-87f5-4b4c-a58a-45d8981ac56d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---+---+----+\n",
            "|Date/Time|Lat|Lon|Base|\n",
            "+---------+---+---+----+\n",
            "|        0|  0|  0|   0|\n",
            "+---------+---+---+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "null = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "null.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4auKSnVFoot",
        "outputId": "e74a9faf-e953-4949-a75a-e9ebb09ee55e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------+---------------+----------+\n",
            "|    Lat|     Lon|Date/Time_index|Base_index|\n",
            "+-------+--------+---------------+----------+\n",
            "|40.7366|-73.9906|        20173.0|       4.0|\n",
            "| 40.726|-73.9918|        34403.0|       4.0|\n",
            "|40.7209|-74.0507|        25098.0|       4.0|\n",
            "|40.7387|-73.9856|        25098.0|       4.0|\n",
            "|40.7323|-74.0077|        25098.0|       4.0|\n",
            "|40.7349|-74.0033|        29513.0|       4.0|\n",
            "|40.7279|-73.9542|        20174.0|       4.0|\n",
            "| 40.721|-73.9937|        29515.0|       4.0|\n",
            "|40.7195| -74.006|        26643.0|       4.0|\n",
            "|40.7448|-73.9799|        29516.0|       4.0|\n",
            "|40.7399|-74.0057|        32031.0|       4.0|\n",
            "|40.7651|-73.9683|        29518.0|       4.0|\n",
            "|40.7354|-74.0081|        30819.0|       4.0|\n",
            "|40.7339|-74.0028|        20175.0|       4.0|\n",
            "|40.7364|-74.0301|        20175.0|       4.0|\n",
            "|40.7364|-74.0301|        20175.0|       4.0|\n",
            "|40.7252|-73.9516|        33228.0|       4.0|\n",
            "|40.7433| -73.986|        33228.0|       4.0|\n",
            "|40.7437|-73.9884|        26644.0|       4.0|\n",
            "|40.7406|-74.0077|        32034.0|       4.0|\n",
            "+-------+--------+---------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "string_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
        "indexers = {c: StringIndexer(inputCol=c, outputCol=c+\"_index\") for c in string_cols}\n",
        "indexed_df = df\n",
        "for c, indexer in indexers.items():\n",
        "    indexed_df = indexer.fit(indexed_df).transform(indexed_df)\n",
        "numeric_cols = [c for c, t in indexed_df.dtypes if t in [\"int\", \"double\", \"float\", \"long\"]]\n",
        "numeric_df = indexed_df.select(numeric_cols)\n",
        "numeric_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohRY4iXkGJH0"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(inputCols=['Lat', 'Lon'], outputCol='features')\n",
        "data = assembler.transform(df.select('Lat', 'Lon'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC9W0k6JH1FC"
      },
      "outputs": [],
      "source": [
        "(trainingData, testData) = data.randomSplit([0.8, 0.2], seed=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4oD3gBYBejE"
      },
      "outputs": [],
      "source": [
        "testdatatocsv = testData.select('Lat','Lon')\n",
        "testdatatocsv.write.csv('test.csv', header=True, mode='overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install kafka-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sW1kwA0IGXz5",
        "outputId": "86749919-9f03-4ffd-a2a4-caf579984086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import json\n",
        "#from pyspark.sql.functions import to_json, struct\n",
        "#from time import sleep\n",
        "#from kafka import KafkaProducer\n",
        "\n",
        "#producer = KafkaProducer(bootstrap_servers=['localhost:9092'], \n",
        "#                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
        "\n",
        "#df_json = testData.select(to_json(struct([testData[x] for x in testData.columns])).alias(\"value\")).collect()\n",
        "\n",
        "#for data in df_json:\n",
        "#    producer.send('uber_data', value=data['value'])\n",
        "#    sleep(1)\n",
        "#    print(f\"{data['value']}\")\n"
      ],
      "metadata": {
        "id": "HDyQAMQCGms4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost = []\n",
        "for i in range(2, 11):\n",
        "    kmeans = KMeans(k=i, seed=100)\n",
        "    model = kmeans.fit(data.select('Lat', 'Lon' , 'features'))\n",
        "    predictions = model.transform(data)\n",
        "    evaluator = ClusteringEvaluator()\n",
        "    silhouette = evaluator.evaluate(predictions)\n",
        "    print(f'Cost {i} : ' + str(silhouette))\n",
        "    cost.append(silhouette)  \n",
        "#model.write().overwrite().save(\"kmeans_model\")    \n",
        "    "
      ],
      "metadata": {
        "id": "bvt8zeJKf8yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(2, 11), cost)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-XXbjtKhgAVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=[\"Lat\", \"Lon\"], outputCol=\"features\")\n",
        "trainingData = assembler.transform(trainingData).select(\"features\")\n",
        "kmeans = KMeans().setK(5).setSeed(20)\n",
        "model = kmeans.fit(trainingData)\n",
        "model.save(\"kmeans5.model\")"
      ],
      "metadata": {
        "id": "Z32bMhnwHRiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oriRQ8zPhsu6"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans().setK(5).setSeed(20)\n",
        "model = kmeans.fit(trainingData)\n",
        "model.save(\"kmeans5.model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from confluent_kafka import Producer, KafkaError\n",
        "\n",
        "# تنظیمات کانفیگ کافکا\n",
        "#conf = {\n",
        "  #  'bootstrap.servers': 'localhost:9092', # آدرس سرورهای بوت استرپ\n",
        " #   'client.id': 'python-producer'\n",
        "#}\n",
        "\n",
        "# ایجاد یک تولیدکننده کافکا\n",
        "#producer = Producer(conf)\n",
        "#file_path='/content/output.csv/part-00000-be01dc14-085d-4a48-9cc5-75caf43aa79b-c000.csv'\n",
        "# باز کردن فایل CSV و خواندن داده‌ها\n",
        "#with open(file_path, 'r') as f:\n",
        " #   for line in f:\n",
        " #       # فرستادن هر خط به کافکا به عنوان یک پیام\n",
        "  #      producer.produce('my_topic', line.encode('utf-8'))\n",
        "#\n",
        "# فلش کردن پیام‌ها در کافکا\n",
        "#producer.flush()"
      ],
      "metadata": {
        "id": "m7bSceNR9yhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark.streaming import StreamingContext\n",
        "#from pyspark.streaming.kafka import KafkaUtils\n",
        "# تعریف تابع پردازش برای استریم\n",
        "#def process_stream(stream):\n",
        "    # پردازش داده های رسیده به استریم\n",
        "#    stream.pprint()\n",
        "\n",
        "# تنظیمات اسپارک\n",
        "#appName = \"KafkaSparkStream\"\n",
        "#master = \"local[*]\"\n",
        "#batchDuration = 5\n",
        "\n",
        "# تنظیمات کافکا\n",
        "#kafkaParams = {\n",
        "  #  \"metadata.broker.list\": \"localhost:9092\",\n",
        " #   \"auto.offset.reset\": \"smallest\"\n",
        "#}\n",
        "#topic = \"my_topic\"\n",
        "\n",
        "# ایجاد شیء StreamingContext\n",
        "#ssc = StreamingContext(master, batchDuration)\n",
        "\n",
        "# خواندن داده های کافکا به عنوان یک DStream\n",
        "#kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], kafkaParams)\n",
        "\n",
        "# پردازش داده های دریافت شده از کافکا\n",
        "#kafkaStream.foreachRDD(lambda rdd: process_stream(rdd))\n",
        "\n",
        "# شروع استریم\n",
        "#ssc.start()\n",
        "#ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "5D602jw5928R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izRFIl5l_Adj"
      },
      "outputs": [],
      "source": [
        "model.save('kmeans')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nuFnZuAE3Bd",
        "outputId": "c48eb791-4179-449f-8d1f-0acdabb20744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------+------------------+----------+\n",
            "|    Lat|     Lon|          features|prediction|\n",
            "+-------+--------+------------------+----------+\n",
            "|40.7366|-73.9906|[40.7366,-73.9906]|         4|\n",
            "| 40.726|-73.9918| [40.726,-73.9918]|         4|\n",
            "|40.7209|-74.0507|[40.7209,-74.0507]|         4|\n",
            "|40.7387|-73.9856|[40.7387,-73.9856]|         0|\n",
            "|40.7323|-74.0077|[40.7323,-74.0077]|         4|\n",
            "|40.7349|-74.0033|[40.7349,-74.0033]|         4|\n",
            "|40.7279|-73.9542|[40.7279,-73.9542]|         0|\n",
            "| 40.721|-73.9937| [40.721,-73.9937]|         4|\n",
            "|40.7195| -74.006| [40.7195,-74.006]|         4|\n",
            "|40.7448|-73.9799|[40.7448,-73.9799]|         0|\n",
            "|40.7399|-74.0057|[40.7399,-74.0057]|         4|\n",
            "|40.7651|-73.9683|[40.7651,-73.9683]|         0|\n",
            "|40.7354|-74.0081|[40.7354,-74.0081]|         4|\n",
            "|40.7339|-74.0028|[40.7339,-74.0028]|         4|\n",
            "|40.7364|-74.0301|[40.7364,-74.0301]|         4|\n",
            "|40.7364|-74.0301|[40.7364,-74.0301]|         4|\n",
            "|40.7252|-73.9516|[40.7252,-73.9516]|         8|\n",
            "|40.7433| -73.986| [40.7433,-73.986]|         0|\n",
            "|40.7437|-73.9884|[40.7437,-73.9884]|         0|\n",
            "|40.7406|-74.0077|[40.7406,-74.0077]|         4|\n",
            "+-------+--------+------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8Uz5vCF87Rv",
        "outputId": "a853c2f2-9635-4559-b9fc-44bd87168ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Date/Time', 'string'), ('Lat', 'double'), ('Lon', 'double'), ('Base', 'string')]\n"
          ]
        }
      ],
      "source": [
        "#import pandas as pd\n",
        "#print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0bL-s6zM4de",
        "outputId": "ab8f5d43-b272-4abf-bca8-58154037f196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark.sql import SparkSession\n",
        "#from pyspark.sql.functions import *\n",
        "\n",
        "#spark = SparkSession.builder.appName(\"csv_to_kafka\").getOrCreate()\n",
        "\n",
        "# خواندن داده های CSV\n",
        "#df = spark.read.format(\"uber_data.csv\").option(\"header\", \"true\").load(\"output.csv\")\n",
        "\n",
        "# تبدیل داده ها به رشته JSON\n",
        "#df_json = df.select(to_json(struct(\"*\")).alias(\"value\"))\n",
        "\n",
        "# ارسال داده ها به Kafka\n",
        "#df_json.write.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"topic\", \"test_topic\").save()\n",
        "\n",
        "# بستن SparkSession\n",
        "#spark.stop()"
      ],
      "metadata": {
        "id": "IZQdavhhgoaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls -r"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV-z1mjiDSHB",
        "outputId": "f7389e25-5c51-439c-ff19-97ac30515054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "uber-raw-data-aug14.csv  sample_data  output.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}